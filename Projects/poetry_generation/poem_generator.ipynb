{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c6a99e",
   "metadata": {},
   "source": [
    "# Poetry Generation with LSTM & GRU (Keras)\n",
    "This notebook explores how to build and train a Recurrent Neural Network (RNN) using LSTM and GRU layers to generate poetry. \n",
    "\n",
    "Models are trained on a custom dataset of poems and learns to predict and generate poetic lines based on input text.\n",
    "\n",
    "- Framework: TensorFlow / Keras  \n",
    "- Architecture: Word-level LSTM / GRU \n",
    "- Dataset: Text file (`poem.txt`) https://www.kaggle.com/datasets/harshalgadhe/poem-generation\n",
    "- Goal: Generate poetry using AI with adjustable creativity (temperature sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15d06c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c291d40",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69391b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the TXT file\n",
    "with open(\"poem.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa9e1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay, i said\n",
      "to the cut flowers.\n",
      "they bowed\n",
      "their heads lower.\n",
      "stay, i said to the spider,\n",
      "who fled.\n",
      "stay, leaf.\n",
      "it reddened,\n",
      "embarrassed for me and itself.\n",
      "stay, i said to my body.\n",
      "it sat as a dog do\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7311f",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a05a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 3808\n",
      "Total training sequences: 16311\n"
     ]
    }
   ],
   "source": [
    "# create instance of tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit tokenizer to current text\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(f\"Total unique words: {total_words}\")\n",
    "\n",
    "# # Generate input sequences using n-gram approach\n",
    "input_sequences = []\n",
    "for line in text.split(\"\\n\"):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(f\"Total training sequences: {len(input_sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfe6c",
   "metadata": {},
   "source": [
    "## Pad Sequences and Prepare Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf544d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# x contains all words except  the last word (to be generated)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1] # word to be generated for each line\n",
    "y = np.eye(total_words)[y]  # one-hot encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbdbd25",
   "metadata": {},
   "source": [
    "## Build and Train First GRU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f281c030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 15, 100)           380800    \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 15, 128)           88320     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 15, 128)           0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 128)               99072     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3808)              491232    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,059,424\n",
      "Trainable params: 1,059,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "gru_model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_seq_len - 1),\n",
    "    GRU(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "gru_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f8a6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "510/510 [==============================] - 12s 13ms/step - loss: 7.0930 - accuracy: 0.0611\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 6.5906 - accuracy: 0.0661\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 6.3486 - accuracy: 0.0764\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 6.1170 - accuracy: 0.0891\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 5.8652 - accuracy: 0.0994\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 5.5945 - accuracy: 0.1116\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 5.3271 - accuracy: 0.1208\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 5.0481 - accuracy: 0.1351\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.7744 - accuracy: 0.1498\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.5107 - accuracy: 0.1725\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.2731 - accuracy: 0.1986\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.0331 - accuracy: 0.2307\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 3.8237 - accuracy: 0.2600\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 3.6341 - accuracy: 0.2864\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 3.4456 - accuracy: 0.3183\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 3.2770 - accuracy: 0.3430\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 3.1244 - accuracy: 0.3725\n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.9908 - accuracy: 0.3882\n",
      "Epoch 19/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 2.8562 - accuracy: 0.4134\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 2.7442 - accuracy: 0.4293\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.6288 - accuracy: 0.4524\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.5464 - accuracy: 0.4662\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.4616 - accuracy: 0.4830\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.3741 - accuracy: 0.4981\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.2974 - accuracy: 0.5092\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.2317 - accuracy: 0.5236\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.1567 - accuracy: 0.5323\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.0949 - accuracy: 0.5435\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 2.0415 - accuracy: 0.5553\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.9874 - accuracy: 0.5648\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.9294 - accuracy: 0.5767\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.8838 - accuracy: 0.5852\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.8390 - accuracy: 0.5901\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.7954 - accuracy: 0.5991\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.7598 - accuracy: 0.6054\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.7291 - accuracy: 0.6064\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.6991 - accuracy: 0.6197\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.6544 - accuracy: 0.6254\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.6392 - accuracy: 0.6278\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.5996 - accuracy: 0.6401\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.5744 - accuracy: 0.6390\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.5503 - accuracy: 0.6441\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.5155 - accuracy: 0.6543\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.4942 - accuracy: 0.6568\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.4686 - accuracy: 0.6627\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.4504 - accuracy: 0.6683\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.4269 - accuracy: 0.6693\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.4035 - accuracy: 0.6759\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.3842 - accuracy: 0.6806\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.3681 - accuracy: 0.6802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d81c38a680>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "gru_model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06eaa2e",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8e2f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defien funcitons to generate text using trained model\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "def generate_poem(seed_text, model, next_words=30, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = sample_with_temperature(predicted, temperature)\n",
    "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb89df93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i said to myself and has only with tease me out of thought with a summers wall to the wearin o the green and\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(\"i said to myself\", model=gru_model, next_words=20, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715cb61",
   "metadata": {},
   "source": [
    "##### model doesn't seem to be doing well, based on accuracy, it looks to underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499754d",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ff4ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 15, 100)           380800    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 15, 128)           117248    \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 15, 128)           0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3808)              491232    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,120,864\n",
      "Trainable params: 1,120,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "510/510 [==============================] - 11s 15ms/step - loss: 6.9312 - accuracy: 0.0618\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 6.5654 - accuracy: 0.0630\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 6.4668 - accuracy: 0.0707\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 6.3667 - accuracy: 0.0788\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 6.2451 - accuracy: 0.0842\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 6.0980 - accuracy: 0.0942\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.9606 - accuracy: 0.0991\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.8440 - accuracy: 0.1042\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.7377 - accuracy: 0.1061\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.6375 - accuracy: 0.1105\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.5447 - accuracy: 0.1129\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.4560 - accuracy: 0.1178\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.3654 - accuracy: 0.1194\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.2691 - accuracy: 0.1228\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.1747 - accuracy: 0.1262\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 5.0754 - accuracy: 0.1278\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.9885 - accuracy: 0.1322\n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.8941 - accuracy: 0.1362\n",
      "Epoch 19/50\n",
      "510/510 [==============================] - 7s 13ms/step - loss: 4.7902 - accuracy: 0.1430\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.6967 - accuracy: 0.1451\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.6074 - accuracy: 0.1520\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.5227 - accuracy: 0.1538\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.4328 - accuracy: 0.1614\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.3446 - accuracy: 0.1646\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.2635 - accuracy: 0.1722\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.1863 - accuracy: 0.1782\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.0997 - accuracy: 0.1847\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 4.0335 - accuracy: 0.1953\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.9520 - accuracy: 0.2058\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.8848 - accuracy: 0.2097\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.8168 - accuracy: 0.2188\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.7599 - accuracy: 0.2295\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.6953 - accuracy: 0.2389\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 3.6368 - accuracy: 0.2495\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 15s 30ms/step - loss: 3.5767 - accuracy: 0.2581\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 8s 16ms/step - loss: 3.5234 - accuracy: 0.2658\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 3.4659 - accuracy: 0.2725\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 3.4198 - accuracy: 0.2809\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 3.3850 - accuracy: 0.2859\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 3.3279 - accuracy: 0.2934\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 3.2856 - accuracy: 0.3012\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 3.2419 - accuracy: 0.3077\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 3.2054 - accuracy: 0.3157\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 3.1707 - accuracy: 0.3166\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 3.1159 - accuracy: 0.3317\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 3.0938 - accuracy: 0.3333\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 3.0559 - accuracy: 0.3395\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 3.0130 - accuracy: 0.3447\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 2.9752 - accuracy: 0.3534\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 2.9543 - accuracy: 0.3529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d81c95e710>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm model with same architecture as gru model, and early stopping applied\n",
    "lstm_model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_seq_len - 1),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dropout(0.3),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(monitor='loss',patience = 5)\n",
    "\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.fit(X, y, epochs=50, verbose=1, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1966e0f",
   "metadata": {},
   "source": [
    "#### lstm_model looks to underfit even more than the gru model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c41da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i said to myself to van diemans land is a jewel when so deep as gone to darlin troubled gone wandered gone gone by\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(\"i said to myself\", model=lstm_model, next_words=20, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae217383",
   "metadata": {},
   "source": [
    "## Bi-directional GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f1d96de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 15, 100)           380800    \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 15, 256)          176640    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 15, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 256)              296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3808)              978656    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,832,544\n",
      "Trainable params: 1,832,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "gru_model2 = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_seq_len - 1),\n",
    "    Bidirectional(GRU(128, return_sequences=True)),\n",
    "    Dropout(0.4),\n",
    "    Bidirectional(GRU(128)),\n",
    "    Dropout(0.4),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(monitor='loss',patience = 5)\n",
    "\n",
    "gru_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gru_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ddf588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "510/510 [==============================] - 9s 11ms/step - loss: 6.8761 - accuracy: 0.0641\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 6.4228 - accuracy: 0.0777\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 6.1512 - accuracy: 0.0927\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 5.8980 - accuracy: 0.1066\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 5.6253 - accuracy: 0.1192\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 5.3588 - accuracy: 0.1343\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 5.0597 - accuracy: 0.1504\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.7640 - accuracy: 0.1688\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 4.4806 - accuracy: 0.1925\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 4.2047 - accuracy: 0.2208\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 3.9321 - accuracy: 0.2500\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 3.6886 - accuracy: 0.2799\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 3.4576 - accuracy: 0.3138\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 15s 30ms/step - loss: 3.2352 - accuracy: 0.3448\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 3.0328 - accuracy: 0.3807\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 16s 32ms/step - loss: 2.8510 - accuracy: 0.4122\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 2.6967 - accuracy: 0.4376\n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 16s 32ms/step - loss: 2.5490 - accuracy: 0.4624\n",
      "Epoch 19/50\n",
      "510/510 [==============================] - 16s 31ms/step - loss: 2.4124 - accuracy: 0.4801\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 17s 33ms/step - loss: 2.3088 - accuracy: 0.4988\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 16s 31ms/step - loss: 2.1891 - accuracy: 0.5242\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 2.0926 - accuracy: 0.5429\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 2.0154 - accuracy: 0.5529\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 1.9416 - accuracy: 0.5706\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 1.8636 - accuracy: 0.5807\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 1.8098 - accuracy: 0.5924\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 1.7357 - accuracy: 0.6057\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 1.6994 - accuracy: 0.6141\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 1.6479 - accuracy: 0.6253\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 1.6053 - accuracy: 0.6349\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 1.5476 - accuracy: 0.6477\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 1.5187 - accuracy: 0.6494\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 1.4917 - accuracy: 0.6588\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.4453 - accuracy: 0.6687\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.4251 - accuracy: 0.6707\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.3905 - accuracy: 0.6808\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 1.3607 - accuracy: 0.6839\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 1.3426 - accuracy: 0.6852\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 1.3268 - accuracy: 0.6876\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 1.3044 - accuracy: 0.6950\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 1.2763 - accuracy: 0.7050\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 1.2569 - accuracy: 0.7088\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.2479 - accuracy: 0.7086\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.2273 - accuracy: 0.7139\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1975 - accuracy: 0.7226\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1934 - accuracy: 0.7209\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1746 - accuracy: 0.7255\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1484 - accuracy: 0.7334\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1394 - accuracy: 0.7351\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 1.1394 - accuracy: 0.7329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d85f7bf9a0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "gru_model2.fit(X, y, epochs=50, verbose=1, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "467af795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i said to myself i see another free goodbye to dear old skibbereen and killarney come home they took a tired boy i love\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(\"i said to myself\", model=gru_model2, next_words=20, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c1f7a",
   "metadata": {},
   "source": [
    "#### looks to perform better than previous gru model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019bc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
